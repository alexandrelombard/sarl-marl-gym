/** 
 * 
 */
package fr.ciadlab.lanechange.^agent

import fr.ciadlab.gym.IEnvironment
import fr.ciadlab.gym.SimulationStep
import fr.ciadlab.gym.capacities.EnvironmentAction
import fr.ciadlab.gym.capacities.EnvironmentStatePerception
import fr.ciadlab.lanechange.env.EnvironmentMDP
import io.sarl.core.Initialize
import org.deeplearning4j.rl4j.learning.sync.qlearning.QLearning
import org.deeplearning4j.rl4j.learning.sync.qlearning.discrete.QLearningDiscreteDense
import org.deeplearning4j.rl4j.network.dqn.DQNFactoryStdDense
import org.deeplearning4j.rl4j.util.DataManager
import org.nd4j.linalg.learning.config.Adam

/** 
 * 
 * @author Alexandre Lombard
 * 
 */
behavior RLAgentBehavior {
	uses EnvironmentAction, EnvironmentStatePerception
	
	val RL_AGENT_QL = new QLearning.QLConfiguration(
			123, 	//Random seed
			200, 	// Max step By epoch
            150000, // Max step
            150000, // Max size of experience replay
            32, 	// size of batches
            500, 	// target update (hard)
            10, 	// num step noop warmup
            0.01, 	// reward scaling
            0.99, 	// gamma
            1.0, 	// td-error clipping
            0.1f, 	// min epsilon
            1000, 	// num step for eps greedy anneal
            true 	// double DQN
            )
            
	val RL_AGENT_NET = DQNFactoryStdDense.Configuration.builder().l2(0.001)
			.updater(new Adam(0.0005))
			.numHiddenNodes(16)
			.numLayer(3)
			.build()
	
	on Initialize {
		val environment = occurrence.parameters.get(0) as IEnvironment
		
		val dataManager = new DataManager(true)
		val mdp = new EnvironmentMDP(environment, this.ID)
		val dql = new QLearningDiscreteDense(mdp, RL_AGENT_NET, RL_AGENT_QL, dataManager)
	}

	on SimulationStep {
		
	}
	

}
